{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Model Evolution & Evaluation\n",
    "\n",
    "> **PM Accelerator Mission**: \"By making industry-leading tools and education available to individuals from all backgrounds, we level the playing field for future PM leaders.\"\n",
    "\n",
    "This notebook covers:\n",
    "1. Model Architecture Comparison\n",
    "2. Training Process\n",
    "3. Evaluation Metrics\n",
    "4. Final Model Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Evolution Journey\n",
    "\n",
    "### ğŸ“ˆ Performance Timeline\n",
    "\n",
    "| Version | Model | MAE | Parameters | Training Time |\n",
    "|---------|-------|-----|------------|---------------|\n",
    "| V1.0 | MLP (3 layers) | ~4.5Â°C | ~50K | 5 min |\n",
    "| V2.2 | LSTM (2 layers) | 2.05Â°C | ~100K | 30 min |\n",
    "| V2.3 | Transformer (4 layers) | 2.05Â°C | ~200K | 15 min |\n",
    "| V3.0 | Multivariate Transformer | 2.07Â°C | ~250K | 20 min |\n",
    "| **V4.0** | **Advanced Transformer + GRN** | **2.00Â°C** | **1.3M** | **45 min** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Model evolution visualization\n",
    "versions = ['V1 MLP', 'V2.2 LSTM', 'V2.3 Transformer', 'V3.0 Multivariate', 'V4.0 Advanced']\n",
    "mae_scores = [4.5, 2.05, 2.05, 2.07, 2.00]\n",
    "colors = ['#ff6b6b', '#ffd93d', '#6bcb77', '#4d96ff', '#00f296']\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(x=versions, y=mae_scores, marker_color=colors, text=[f'{m}Â°C' for m in mae_scores], textposition='outside')\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='ğŸ“Š Model Performance Evolution (Lower is Better)',\n",
    "    yaxis_title='MAE (Â°C)',\n",
    "    template='plotly_dark',\n",
    "    yaxis_range=[0, 5]\n",
    ")\n",
    "fig.add_hline(y=2.0, line_dash='dash', line_color='green', annotation_text='Target: 2.0Â°C')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture Deep Dives\n",
    "\n",
    "### V1: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "```\n",
    "Input (20 features) â†’ Dense(256, ReLU) â†’ Dense(128, ReLU) â†’ Dense(64, ReLU) â†’ Output(7)\n",
    "```\n",
    "\n",
    "**Pros:**\n",
    "- Simple, fast to train\n",
    "- Easy to interpret\n",
    "\n",
    "**Cons:**\n",
    "- No temporal awareness\n",
    "- Cannot capture sequences\n",
    "\n",
    "---\n",
    "\n",
    "### V2.2: LSTM (Long Short-Term Memory)\n",
    "\n",
    "```\n",
    "Input Sequence (30 days) â†’ LSTM(128) â†’ LSTM(128) â†’ Dense(64) â†’ Output(7)\n",
    "```\n",
    "\n",
    "**Innovation:**\n",
    "- First model to use **30-day historical sequence**\n",
    "- Remembers patterns across days\n",
    "\n",
    "**Pros:**\n",
    "- Captures temporal dependencies\n",
    "- Good at short-term patterns\n",
    "\n",
    "**Cons:**\n",
    "- Slow training (sequential nature)\n",
    "- Struggles with long-range dependencies\n",
    "\n",
    "---\n",
    "\n",
    "### V2.3: Transformer\n",
    "\n",
    "```\n",
    "Input (30 days) â†’ Positional Encoding â†’ Transformer Encoder (4 layers) â†’ Output Head â†’ 7 days\n",
    "```\n",
    "\n",
    "**Innovation:**\n",
    "- **Self-Attention**: Every day can attend to every other day\n",
    "- **Parallelizable**: Much faster training\n",
    "\n",
    "**Pros:**\n",
    "- Captures long-range dependencies\n",
    "- Faster than LSTM\n",
    "\n",
    "**Cons:**\n",
    "- Treats all features equally (no importance weighting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V4.0: Advanced Transformer with GRN (Final Model)\n",
    "\n",
    "```\n",
    "Input (30 days Ã— 25 features)\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Gated Residual Networkâ”‚  â† Learns to skip irrelevant inputs\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Positional Encoding  â”‚  â† Injects sequence order\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Transformer Encoder  â”‚  â† 6 layers, 8 attention heads\n",
    "â”‚   (Self-Attention)    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Gated Residual Networkâ”‚  â† Filters noise before output\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "    7-Day Forecast\n",
    "```\n",
    "\n",
    "### What is a Gated Residual Network (GRN)?\n",
    "\n",
    "The GRN is a key innovation that allows the model to **selectively ignore irrelevant features**.\n",
    "\n",
    "```python\n",
    "# Simplified GRN concept:\n",
    "def grn(x):\n",
    "    hidden = elu(fc1(x))\n",
    "    output = fc2(hidden)\n",
    "    gate = sigmoid(fc_gate(hidden))  # 0-1 importance weight\n",
    "    return layer_norm(gate * output + skip(x))  # Skip connection!\n",
    "```\n",
    "\n",
    "**Key Benefits:**\n",
    "1. **Feature Importance**: The gate learns which features matter\n",
    "2. **Skip Connections**: If a feature is useless, it can be bypassed\n",
    "3. **Noise Filtering**: Reduces impact of noisy weather data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V4 Training Configuration\n",
    "config = {\n",
    "    'architecture': 'AdvancedWeatherTransformer',\n",
    "    'd_model': 128,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'dropout': 0.15,\n",
    "    'sequence_length': 30,\n",
    "    'prediction_length': 7,\n",
    "    'features': 25,\n",
    "    'parameters': '1,324,167',\n",
    "    'loss_function': 'HuberLoss (delta=1.0)',\n",
    "    'optimizer': 'AdamW (lr=0.001, weight_decay=0.02)',\n",
    "    'scheduler': 'CosineAnnealingWarmRestarts (T_0=10)',\n",
    "    'batch_size': 256,\n",
    "    'epochs': 44,  # Early stopped from 150\n",
    "    'gradient_clipping': 0.5\n",
    "}\n",
    "\n",
    "print(\"ğŸ”§ V4 Model Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"   {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why HuberLoss?\n",
    "\n",
    "Standard MSE (Mean Squared Error) heavily penalizes outliers:\n",
    "- If actual temp is 20Â°C and prediction is 50Â°C, MSE = 900!\n",
    "\n",
    "HuberLoss combines MSE and MAE:\n",
    "- For small errors: Behaves like MSE (smooth gradient)\n",
    "- For large errors: Behaves like MAE (less penalty)\n",
    "\n",
    "This makes training **robust to outliers** in weather data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-day MAE for V4 model\n",
    "days = ['Day 1', 'Day 2', 'Day 3', 'Day 4', 'Day 5', 'Day 6', 'Day 7']\n",
    "mae_per_day = [1.97, 1.95, 1.94, 1.94, 1.99, 2.05, 2.14]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatter(x=days, y=mae_per_day, mode='lines+markers+text', \n",
    "               text=[f'{m}Â°C' for m in mae_per_day], textposition='top center',\n",
    "               line=dict(color='#4facfe', width=3),\n",
    "               marker=dict(size=12))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='ğŸ“ˆ MAE by Forecast Day (V4 Model)',\n",
    "    xaxis_title='Forecast Day',\n",
    "    yaxis_title='MAE (Â°C)',\n",
    "    template='plotly_dark',\n",
    "    yaxis_range=[1.5, 2.5]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Key Observation:\")\n",
    "print(\"   - Days 1-4: Very accurate (~1.95Â°C)\")\n",
    "print(\"   - Days 5-7: Slight degradation (~2.1Â°C)\")\n",
    "print(\"   - This is expected: uncertainty grows with forecast horizon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Learnings\n",
    "\n",
    "### What Worked Well\n",
    "1. **Sequence Modeling**: Moving from MLP to LSTM cut MAE in half\n",
    "2. **Gated Residual Networks**: Allowed selective feature importance\n",
    "3. **Real Weather Data**: Open-Meteo integration improved generalization\n",
    "4. **Cyclical Encoding**: Sin/cos for time features\n",
    "\n",
    "### What Didn't Work\n",
    "1. **Adding more features naively**: V3 multivariate was WORSE than V2.3\n",
    "   - Lesson: Need proper feature selection/gating\n",
    "2. **Larger batch sizes**: Beyond 256 hurt convergence\n",
    "3. **Very deep models (>8 layers)**: Overfitting increased\n",
    "\n",
    "### Future Improvements\n",
    "1. **Elevation Data**: Temperature drops ~6.5Â°C per 1000m altitude\n",
    "2. **Ocean Proximity**: Coastal regions have more stable temperatures\n",
    "3. **Real-time Weather API**: Use forecast data for initial conditions\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Conclusion\n",
    "\n",
    "We successfully built a **state-of-the-art weather forecasting system** that:\n",
    "- Achieves **2.00Â°C MAE** (exceeded 2.5Â°C target)\n",
    "- Works for **any location on Earth**\n",
    "- Provides **real-time predictions** via web app\n",
    "- Is **production-ready** with Docker deployment\n",
    "\n",
    "The journey from V1 (4.5Â°C MAE) to V4 (2.0Â°C MAE) demonstrates the power of iterative model improvement and proper feature engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
