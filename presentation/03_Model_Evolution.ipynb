{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Model Evolution & Evaluation\n",
    "\n",
    "> **PM Accelerator Mission**: \"By making industry-leading tools and education available to individuals from all backgrounds, we level the playing field for future PM leaders.\"\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/moazmo/weather-trend-forecasting/blob/main/presentation/03_Model_Evolution.ipynb)\n",
    "[![nbviewer](https://img.shields.io/badge/render-nbviewer-orange.svg)](https://nbviewer.org/github/moazmo/weather-trend-forecasting/blob/main/presentation/03_Model_Evolution.ipynb)\n",
    "\n",
    "This notebook covers:\n",
    "1. Model Architecture Comparison\n",
    "2. Training Process\n",
    "3. Evaluation Metrics\n",
    "4. Final Model Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Evolution Journey\n",
    "\n",
    "### ğŸ“ˆ Performance Timeline\n",
    "\n",
    "| Version | Model | MAE | Parameters | Training Time |\n",
    "|---------|-------|-----|------------|---------------|\n",
    "| V1.0 | MLP (3 layers) | ~4.5Â°C | ~50K | 5 min |\n",
    "| V2.2 | LSTM (2 layers) | 2.05Â°C | ~100K | 30 min |\n",
    "| V2.3 | Transformer (4 layers) | 2.05Â°C | ~200K | 15 min |\n",
    "| V3.0 | Multivariate Transformer | 2.07Â°C | ~250K | 20 min |\n",
    "| **V4.0** | **Advanced Transformer + GRN** | **2.00Â°C** | **1.3M** | **45 min** |\n",
    "\n",
    "![Model Evolution](images/model_evolution.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# Model evolution visualization (interactive version)\n",
    "versions = ['V1 MLP', 'V2.2 LSTM', 'V2.3 Transformer', 'V3.0 Multivariate', 'V4.0 Advanced']\n",
    "mae_scores = [4.5, 2.05, 2.05, 2.07, 2.00]\n",
    "colors = ['#ff6b6b', '#ffd93d', '#6bcb77', '#4d96ff', '#00f296']\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(x=versions, y=mae_scores, marker_color=colors, text=[f'{m}Â°C' for m in mae_scores], textposition='outside')\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='ğŸ“Š Model Performance Evolution (Lower is Better)',\n",
    "    yaxis_title='MAE (Â°C)',\n",
    "    template='plotly_dark',\n",
    "    yaxis_range=[0, 5]\n",
    ")\n",
    "fig.add_hline(y=2.0, line_dash='dash', line_color='green', annotation_text='Target: 2.0Â°C')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Architecture Deep Dives\n",
    "\n",
    "### V1: Multi-Layer Perceptron (MLP)\n",
    "\n",
    "```\n",
    "Input (20 features) â†’ Dense(256, ReLU) â†’ Dense(128, ReLU) â†’ Dense(64, ReLU) â†’ Output(7)\n",
    "```\n",
    "\n",
    "**Pros:** Simple, fast to train, easy to interpret\n",
    "\n",
    "**Cons:** No temporal awareness, cannot capture sequences\n",
    "\n",
    "---\n",
    "\n",
    "### V2.2: LSTM (Long Short-Term Memory)\n",
    "\n",
    "```\n",
    "Input Sequence (30 days) â†’ LSTM(128) â†’ LSTM(128) â†’ Dense(64) â†’ Output(7)\n",
    "```\n",
    "\n",
    "**Innovation:** First model to use **30-day historical sequence**\n",
    "\n",
    "---\n",
    "\n",
    "### V4.0: Advanced Transformer with GRN (Final Model)\n",
    "\n",
    "```\n",
    "Input (30 days Ã— 25 features)\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Gated Residual Networkâ”‚  â† Learns to skip irrelevant inputs\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Positional Encoding  â”‚  â† Injects sequence order\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Transformer Encoder  â”‚  â† 6 layers, 8 attention heads\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Gated Residual Networkâ”‚  â† Filters noise before output\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "        â†“\n",
    "    7-Day Forecast\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Configuration\n",
    "\n",
    "| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Architecture | AdvancedWeatherTransformer |\n",
    "| d_model | 128 |\n",
    "| Attention Heads | 8 |\n",
    "| Transformer Layers | 6 |\n",
    "| Dropout | 0.15 |\n",
    "| Parameters | 1,324,167 |\n",
    "| Loss Function | HuberLoss (delta=1.0) |\n",
    "| Optimizer | AdamW (lr=0.001) |\n",
    "| Scheduler | CosineAnnealingWarmRestarts |\n",
    "| Batch Size | 256 |\n",
    "| Epochs | 44 (early stopped) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Results\n",
    "\n",
    "### MAE by Forecast Day\n",
    "\n",
    "![MAE Per Day](images/mae_per_day.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-day MAE visualization (interactive version)\n",
    "days = ['Day 1', 'Day 2', 'Day 3', 'Day 4', 'Day 5', 'Day 6', 'Day 7']\n",
    "mae_per_day = [1.97, 1.95, 1.94, 1.94, 1.99, 2.05, 2.14]\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Scatter(x=days, y=mae_per_day, mode='lines+markers+text', \n",
    "               text=[f'{m}Â°C' for m in mae_per_day], textposition='top center',\n",
    "               line=dict(color='#4facfe', width=3),\n",
    "               marker=dict(size=12))\n",
    "])\n",
    "fig.update_layout(\n",
    "    title='ğŸ“ˆ MAE by Forecast Day (V4 Model)',\n",
    "    xaxis_title='Forecast Day',\n",
    "    yaxis_title='MAE (Â°C)',\n",
    "    template='plotly_dark',\n",
    "    yaxis_range=[1.5, 2.5]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Key Observation:\")\n",
    "print(\"   - Days 1-4: Very accurate (~1.95Â°C)\")\n",
    "print(\"   - Days 5-7: Slight degradation (~2.1Â°C)\")\n",
    "print(\"   - This is expected: uncertainty grows with forecast horizon\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Learnings\n",
    "\n",
    "### âœ… What Worked Well\n",
    "1. **Sequence Modeling**: Moving from MLP to LSTM cut MAE in half\n",
    "2. **Gated Residual Networks**: Allowed selective feature importance\n",
    "3. **Real Weather Data**: Open-Meteo integration improved generalization\n",
    "4. **Cyclical Encoding**: Sin/cos for time features\n",
    "\n",
    "### âŒ What Didn't Work\n",
    "1. **Adding more features naively**: V3 multivariate was WORSE than V2.3\n",
    "2. **Larger batch sizes**: Beyond 256 hurt convergence\n",
    "3. **Very deep models (>8 layers)**: Overfitting increased\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ Conclusion\n",
    "\n",
    "We successfully built a **state-of-the-art weather forecasting system** that:\n",
    "- Achieves **2.00Â°C MAE** (exceeded 2.5Â°C target)\n",
    "- Works for **any location on Earth**\n",
    "- Provides **real-time predictions** via web app\n",
    "- Is **production-ready** with Docker deployment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
